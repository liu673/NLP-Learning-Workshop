{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Word2Vec\n",
    "\n",
    "Word2Vec是一种基于神经网络的embedding模型，由谷歌的研究人员Tomas Mikolov等在2013年提出。Word2Vec的作者认为之前的文本向量化方法很少考虑词与词之间的意义关联，所以效果不佳。因此，Word2Vec将单词映射到低维向量空间中，使得相似词在向量空间中也保持相近的距离。"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<img src=\"images/img_3.png\" alt=\"Image\" style=\"display: block; margin-left: auto; margin-right: auto; width: 500px;\">"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<img src=\"images/img_7.png\" alt=\"Image\" style=\"display: block; margin-left: auto; margin-right: auto; width: 500px;\">"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Word2Vec主要由两种模型架构组成：**Skip-Gram**和**Continuous Bag of Words（CBOW）**。\n",
    "\n",
    "Skip-Gram通过当前词预测其上下文词。给定一个中心词，模型会预测一个固定长度的上下文窗口（前后若干词）中的所有词。训练过程中，模型会学习到每个词的向量表示，使得能够更好地预测这些上下文词。与Skip-Gram相反，CBOW的目标是通过上下文词预测当前词。给定一个上下文窗口中的所有词，模型会预测这个窗口的中心词。\n",
    "\n",
    "Word2Vec模型的核心思想是通过训练神经网络，使得单词与其上下文之间的关系可以在向量空间中被有效地表示。Word2Vec的输入层是一个one-hot向量（one-hot vector），长度为词汇表大小（$V$）。紧接着是一个投影层，由输入层经过一个权重矩阵 $W$（维度为$V \\times N$，$N$为嵌入向量的维度），投影到 $N$ 维向量空间中。投影层的输出通过另一个权重矩阵 $W'$（维度为$N \\times V$），映射回一个词汇表大小的向量，此为输出层。最后经过一个Softmax层得到每个词的概率。在训练过程中，Word2Vec模型通过最大化目标函数（取决于使用Skip-Gram还是CBOW，以及采用的优化函数的方法）来更新权重矩阵，从而使得embedding能够捕捉到词汇的语义信息。\n",
    "\n",
    "> 从监督学习的角度来说，word2vec 本质上是一个基于神经网络的多分类问题，当输出词语非常多时， 我们则需要一些像层级 Softmax 和负采样之类的 trick 来加速训练。但从自然语言处理的角度来说， word2vec 关注的并不是神经网络模型本身，而是训练之后得到的词汇的向量化表征。 这种表征使得最后的词向量维度要远远小于词汇表大小，所以 word2vec 从本质上来说是一种降维操作。 我们把数以万计的词汇从高维空间中降维到低维空间中，大大方便了后续的 NLP 分析任务。\n",
    "\n",
    "\n",
    "## 优缺点\n",
    "\n",
    "优点：\n",
    "● 由于采用了浅层神经网络，训练速度较快，适合处理大规模数据。\n",
    "● 能够捕捉到词与词之间的语义关系，如词性、同义词等。\n",
    "● 可以用于多种自然语言处理任务，如文本分类、情感分析、机器翻译等。\n",
    "\n",
    "缺点：\n",
    "● 每个词只有一个固定的向量表示，无法处理多义词的不同语义。\n",
    "● 无法考虑词在不同上下文中的语义变化。\n",
    "\n",
    "## 应用场景\n",
    "● 信息检索\n",
    "● 文本分类\n",
    "● 推荐系统，如根据用户的评论调整推荐的产品\n",
    "\n",
    "\n",
    "**在Skip-Gram里面，每个词在作为中心词的时候，实际上是 1个学生 VS K个老师，K个老师（周围词）都会对学生（中心词）进行“专业”的训练，这样学生（中心词）的“能力”（向量结果）相对就会扎实（准确）一些，但是这样肯定会使用更长的时间。CBOW是 1个老师 VS K个学生，K个学生（周围词）都会从老师（中心词）那里学习知识，但是老师（中心词）是一视同仁的，教给大家的一样的知识。所以，一般来说 CBOW比Skip-Gram训练速度快，训练过程更加稳定，原因是CBOW使用上下文的方式进行训练，每个训练step会见到更多样本。而在生僻字（出现频率低的字）处理上，skip-gram比CBOW效果更好，学习的词向量更细致，原因就如上面分析：  CBOW 是公共课，Skip-gram 是私教 。**\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## CBOW\n",
    "CBOW 模型的应用场景是要根据上下文预测中间的词，所以输入便是上下文词，当然原始的单词是无法作为输入的， 这里的输入仍然是每个词汇的 One-Hot 向量，输出 Y 为给定词汇表中每个词作为目标词的概率。\n",
    "\n",
    "CBOW 模型结构是一种普通的神经网络结构。主要包括输入层、中间隐藏层、最后的输出层。 以输入、输出样本 $(Context(w),w)$ 为例对 CBOW 模型的三个网络层进行简单说明， 其中假设 Context(w)由 $w$ 前后各 $c$个词构成。\n",
    "  - **输入层**: 包含 $Context(w)$ 中 $2c$ 个词的词向量 $v(Context(w)1),v(Context(w)2),⋯,v(Context(w)2c)\\in{R^m}$。 这里， $m$ 的含义同上表示词向量的长度\n",
    "  - **投影层**: 将输入层的 $2c$ 个向量做求和累加，即 $x_w=\\sum_{i=1}^{2c}v(Context(w)_i)\\in{R^m}$\n",
    "  - **输出层**: 输出层对应一颗二叉树，它是以语料中出现过的词当叶子节点，以各词在语料中出现的次数当权值构造出来的 Huffman 树。在这棵 Huffman 树中， 叶子节点共 $N(=|D|)$ 个，分别对应词典 $D$ 中的词，非叶子节点 $N−1$ 个。\n",
    "\n",
    "普通的基于神经网络的语言模型输出层一般就是利用 softmax 函数进行归一化计算，这种直接 softmax 的做法主要问题在于计算速度， 尤其是我们采用了一个较大的词汇表的时候，对大的词汇表做求和运算，softmax 的分运算会非常慢，直接影响到了模型性能。\n",
    "\n",
    "可以看到，上面提到的取消隐藏层，投影层求和平均都可以一定程度上减少计算量，但输出层的数量在那里， 比如语料库有 500W 个词，那么隐藏层就要对 500W 个神经元进行全连接计算，这依然需要庞大的计算量。 word2vec 算法又在这里进行了训练优化.\n",
    "\n",
    "除了层级 softmax 输出之外，还有一种叫做负采样的训练 trick\n",
    "\n",
    "目标函数：\n",
    "$$\n",
    "J=\\sum_{w \\in corpus} P(w|context(w))\n",
    "$$\n",
    "\n",
    "CBOW 在 NNLM 基础上有以下几点创新\n",
    "\n",
    "1. 取消了隐藏层，减少了计算量\n",
    "2. 采用上下文划窗而不是前文划窗，即用上下文的词来预测当前词\n",
    "3. 投影层不再使用各向量拼接的方式，而是简单的求和平均"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<img src=\"images/img_4.png\" alt=\"Image\" style=\"display: block; margin-left: auto; margin-right: auto; width: 400px;\">"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Skip-Gram\n",
    "\n",
    "Skip-gram 模型的应用场景是要根据中间词预测上下文词，所以输入 $X$ 是任意单词， 输出 $Y$为给定词汇表中每个词作为上下文词的概率。\n",
    "\n",
    "Skip-gram 模型与 CBOW 模型翻转，也是也是一种普通的神经网络结构， 同样也包括输入层、中间隐藏层和最后的输出层。继续以输入输出样本 $(Context(w)，w)$\n",
    " 为例 对 Skip-gram 模型的三个网络层进行简单说明， 其中假设 $Context(w)$ 由 $w$ 前后各 $c$ 个词构成。数学细节如下:\n",
    "- **输入层**: 只含当前样本的中心词 $w$ 的词向量 $v(w)∈Rm$\n",
    "- **投影层**: 这是个恒等投影，把 $v(w)$ 投影到 $v(w)$，因此，这个投影层其实是多余的。 这里之所以保留投影层主要是方便和 CBOW 模型的网络结构做对比\n",
    "- **输出层**: 和 CBOW 模型一样，输出层也是一棵 Huffman 树"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<img src=\"images/img_5.png\" alt=\"Image\" style=\"display: block; margin-left: auto; margin-right: auto; width: 400px;\">"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<img src=\"images/img_6.png\" alt=\"Image\" style=\"display: block; margin-left: auto; margin-right: auto; width: 400px;\">"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "from gensim.models.word2vec import Word2Vec\n",
    "import gensim\n",
    "\n",
    "# gensim.__version__\n",
    "\n",
    "# model = Word2Vec(sentences=topics_list,iter=5, size=128,window=5,min_count=0,workers=10,sg=1,hs=1,negative=1,seed=128,compute_loss=True)\n",
    "\n",
    "# sentences：训练模型的语料，是一个可迭代的序列\n",
    "# corpus_file：表示从文件中加载数据，和sentences互斥\n",
    "# vector_size：word的维度，默认为100，通常取64、128、256等\n",
    "# window：滑动窗口的大小（词向量上下文最大距离），默认值为5。window越大，则和某一词较远的词也会产生上下文关系。在实际使用中，可以根据实际的需求来动态调整这个window的大小。如果是小语料则这个值可以设的更小。对于一般的语料这个值推荐在[5,10]之间。\n",
    "# min_count：word次数小于该值被忽略掉（词频），默认值为5。如果是小语料，可以调低这个值。\n",
    "# seed：用于随机数发生器\n",
    "# workers：使用多少线程进行模型训练，默认为3\n",
    "# min_alpha=0.0001：支持的最小迭代率，默认为0.0001\n",
    "# sg：1 表示 Skip-gram，0 表示 CBOW，默认为0\n",
    "# hs：1 表示 hierarchical softmax ，0 且 negative 参数不为0 的话 negative sampling 会被启用，默认为0\n",
    "# negative：0 表示不采用，1 表示采用，建议值在 5-20 表示噪音词的个数，默认为5\n",
    "# iter：迭代次数，默认为5\n",
    "# compute_loss：是否计算损失值，默认值为False\n",
    "\n",
    "# model.wv.key_to_index 词表到索引\n",
    "# model.wv.index_to_key 所有的词\n",
    "# model.wv.vectors 每个词的词向量\n",
    "# model.wv.vector_size 词向量的维度\n",
    "# model.wv.get_vector(word) 返回给定单词的词向量\n",
    "# model.wv.most_similar(word, topn=10) 返回最相似的10个词\n",
    "# model.wv.similarity(word1, word2) 返回两个词之间的余弦相似度"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "import jieba\n",
    "import re\n",
    "import os\n",
    "from gensim.models import word2vec\n",
    "import time"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "def read_stop(stop_words_path):\n",
    "    \"\"\"\n",
    "    读取停用词\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    # 读取停用词\n",
    "    stop_words = []\n",
    "    with open(stop_words_path, \"r\", encoding=\"utf-8\") as f_reader:\n",
    "        for line in f_reader:\n",
    "            line = line.replace(\"\\r\", \"\").replace(\"\\n\", \"\").strip()\n",
    "            stop_words.append(line)\n",
    "    # print(len(stop_words))\n",
    "    stop_words = set(stop_words)\n",
    "    # print(len(stop_words))\n",
    "    return stop_words"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "def data_process(stop_words, data_path, split_data_path):\n",
    "    \"\"\"\n",
    "    数据预处理\n",
    "    :return:\n",
    "    \"\"\"\n",
    "\n",
    "    # 文本预处理\n",
    "    sentecnces = []\n",
    "    rules = u\"[\\u4e00-\\u9fa5]+\"\n",
    "    pattern = re.compile(rules)\n",
    "    f_writer = open(split_data_path, \"w\", encoding=\"utf-8\")\n",
    "\n",
    "    with open(data_path, \"r\", encoding=\"utf-8\") as f_reader:\n",
    "        for line in f_reader:\n",
    "            line = line.replace(\"\\r\", \"\").replace(\"\\n\", \"\").strip()\n",
    "            if line == \"\" or line is None:\n",
    "                continue\n",
    "            line = \" \".join(jieba.cut(line))\n",
    "            seg_list = pattern.findall(line)\n",
    "            word_list = []\n",
    "            for word in seg_list:\n",
    "                if word not in stop_words:\n",
    "                    word_list.append(word)\n",
    "            if len(word_list) > 0:\n",
    "                sentecnces.append(word_list)\n",
    "                line = \" \".join(word_list)\n",
    "                f_writer.write(line + \"\\n\")\n",
    "                f_writer.flush()\n",
    "    f_writer.close()\n",
    "    return sentecnces"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10961\n"
     ]
    },
    {
     "data": {
      "text/plain": "[['书名', '天龙八部'],\n ['作者', '金庸'],\n ['本文', '早安', '电子书', '网友', '分享', '版权', '原作者'],\n ['用于', '商业行为', '后果自负'],\n ['早安', '电子书'],\n ['金庸', '作品集', '三联', '版', '序'],\n ['小学',\n  '时',\n  '爱读',\n  '课外书',\n  '低年级',\n  '时看',\n  '儿童',\n  '画报',\n  '小朋友',\n  '小学生',\n  '内容',\n  '小朋友',\n  '文库',\n  '似懂非懂',\n  '阅读',\n  '各种各样',\n  '章回小说',\n  '五六年',\n  '级',\n  '时',\n  '看新',\n  '文艺作品',\n  '喜爱',\n  '古典文学',\n  '作品',\n  '多于',\n  '近代',\n  '当代',\n  '新文学',\n  '个性',\n  '使然',\n  '朋友',\n  '喜欢',\n  '新文学',\n  '不爱',\n  '古典文学'],\n ['知识',\n  '当代',\n  '书报',\n  '中',\n  '寻求',\n  '小学',\n  '时代',\n  '得益',\n  '记忆',\n  '最深',\n  '爸爸',\n  '哥哥',\n  '购置',\n  '邹韬奋',\n  '所撰',\n  '萍踪',\n  '寄语',\n  '萍踪',\n  '忆语',\n  '世界各地',\n  '旅行',\n  '记',\n  '主编',\n  '生活',\n  '周报',\n  '新',\n  '旧',\n  '童年时代',\n  '深受',\n  '邹先生',\n  '生活',\n  '书店',\n  '之惠',\n  '生活',\n  '书店',\n  '三联书店',\n  '组成部分',\n  '十多年',\n  '前',\n  '香港三联书店',\n  '签',\n  '合同',\n  '中国',\n  '大陆',\n  '地区',\n  '出版',\n  '小说',\n  '因事',\n  '未果',\n  '重',\n  '行',\n  '筹划',\n  '三联书店',\n  '独家',\n  '出版',\n  '中国',\n  '大陆',\n  '地区',\n  '简体字',\n  '感到',\n  '欣慰',\n  '回忆',\n  '昔日',\n  '心中',\n  '充满',\n  '温馨',\n  '之意'],\n ['撰写',\n  '这套',\n  '总数',\n  '三十六',\n  '册',\n  '作品集',\n  '是从',\n  '一九五五年',\n  '七二年',\n  '约',\n  '十三',\n  '四年',\n  '包括',\n  '十二部',\n  '长篇小说',\n  '两篇',\n  '中篇小说',\n  '一篇',\n  '短篇小说',\n  '一篇',\n  '历史',\n  '人物',\n  '评传',\n  '若干篇',\n  '历史',\n  '考据',\n  '文字',\n  '出版',\n  '过程',\n  '奇怪',\n  '香港',\n  '台湾',\n  '海外',\n  '地区',\n  '中国',\n  '大陆',\n  '先出',\n  '各种各样',\n  '翻版',\n  '盗印',\n  '出版',\n  '校订',\n  '授权',\n  '正',\n  '版本',\n  '中国',\n  '大陆',\n  '三联',\n  '版',\n  '出版',\n  '天津',\n  '百花文艺出版社',\n  '一家',\n  '授权',\n  '出版',\n  '书剑',\n  '恩仇录',\n  '校印',\n  '依足',\n  '合同',\n  '支付',\n  '版税',\n  '依足',\n  '法例',\n  '缴付',\n  '所得税',\n  '余数',\n  '捐给',\n  '几家',\n  '文化',\n  '机构',\n  '支助',\n  '围棋',\n  '活动',\n  '这是',\n  '愉快',\n  '经验',\n  '未经',\n  '授权'],\n ['不付',\n  '版税',\n  '版本',\n  '粗制滥造',\n  '错讹',\n  '百出',\n  '借用',\n  '金庸',\n  '之名',\n  '撰写',\n  '出版',\n  '武侠小说',\n  '写',\n  '不敢掠美',\n  '充满',\n  '无聊',\n  '打斗',\n  '色情',\n  '描写',\n  '之作',\n  '令人',\n  '不快',\n  '出版社',\n  '翻印',\n  '香港',\n  '台湾',\n  '作家',\n  '作品',\n  '而用',\n  '笔名',\n  '出版发行',\n  '收到',\n  '无数',\n  '读者',\n  '来信',\n  '揭露',\n  '大表',\n  '愤慨',\n  '三联',\n  '版',\n  '发行',\n  '制止',\n  '种种',\n  '讲',\n  '道义',\n  '侠义',\n  '小说',\n  '主旨',\n  '讲',\n  '是非',\n  '讲',\n  '道义',\n  '太',\n  '过份']]"
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir_current = os.getcwd()\n",
    "# 注意此处的路径，在 jupyter_notebook_config.py 中设置，否则会报错\n",
    "stop_words_file = os.path.join(dir_current, \"learning_word-vector/data/stop_words.txt\")\n",
    "data_file = os.path.join(dir_current, \"learning_word-vector/data/天龙八部.txt\")\n",
    "split_data_file = os.path.join(dir_current, \"learning_word-vector/data/天龙八部_split.txt\")\n",
    "\n",
    "stop_words = read_stop(stop_words_file)\n",
    "sentecnces = data_process(stop_words, data_file, split_data_file)\n",
    "print(len(sentecnces))\n",
    "sentecnces[:10]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始训练模型...\n",
      "训练模型结束，耗时： 9.789485692977905\n",
      "保存模型到： E:\\Pycharm_Data\\Pycharm_Project\\MyNLP_Project\\NLP-Learning-Workshop\\learning_word-vector/models/天龙八部_word2vec_Skip-gram.bin\n"
     ]
    }
   ],
   "source": [
    "# 模型训练\n",
    "start_time = time.time()\n",
    "print(\"开始训练模型...\")\n",
    "SG = 1 # 0表示CBOW，1表示Skip-gram\n",
    "\n",
    "model = word2vec.Word2Vec(sentecnces, vector_size=128, epochs=50, window=5, min_count=6,\n",
    "                          workers=10, sg=0,\n",
    "                          negative=5, hs=0, seed=42, compute_loss=True,\n",
    "                          )\n",
    "print(\"训练模型结束，耗时：\", time.time() - start_time)\n",
    "# 保存模型\n",
    "if SG == 0:\n",
    "    model_name = \"天龙八部_word2vec_CBOW.bin\"\n",
    "else:\n",
    "    model_name = \"天龙八部_word2vec_Skip-gram.bin\"\n",
    "\n",
    "model_path = os.path.join(dir_current, f\"learning_word-vector/models/{model_name}\")\n",
    "print(\"保存模型到：\", model_path)\n",
    "\n",
    "model.save(model_path)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "乔大爷 0.44025424122810364\n",
      "谭公 0.42235711216926575\n",
      "徐长老 0.4182719886302948\n",
      "宋长老 0.41528820991516113\n",
      "白世镜 0.39926669001579285\n",
      "赵钱孙 0.3771824836730957\n",
      "真相 0.3664160370826721\n",
      "鲍千灵 0.3563803434371948\n",
      "陈长老 0.3414948880672455\n",
      "向乔峰 0.3404667377471924\n",
      "0.2193088\n",
      "0.298908\n"
     ]
    }
   ],
   "source": [
    "# 加载模型\n",
    "model2 = word2vec.Word2Vec.load(model_path)\n",
    "\n",
    "# 选出10个与乔峰最相近的10个词\n",
    "for e in model2.wv.most_similar(positive=[\"乔峰\"], topn=10):\n",
    "    print(e[0], e[1])\n",
    "\n",
    "# 计算两个词语的相似度\n",
    "sim_value = model.wv.similarity('乔峰', '萧峰')\n",
    "print(sim_value)\n",
    "\n",
    "# 计算两个集合的相似度\n",
    "list1 = ['乔峰', '萧远山']\n",
    "list2 = ['慕容复', '慕容博']\n",
    "sim_value = model.wv.n_similarity(list1, list2)\n",
    "print(sim_value)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 学习资源\n",
    "● [Efficient Estimation of Word Representations in Vector Space](https://arxiv.org/abs/1301.3781)\n",
    "\n",
    "● [word2vec Parameter Learning Explained](https://arxiv.org/abs/1411.2738)\n",
    "\n",
    "● [可视化 word2vec](https://ronxin.github.io/wevi/)\n",
    "\n",
    "● **[深入浅出理解word2vec模型 (理论与源码分析)](https://mp.weixin.qq.com/s/mb1MmW5SOXwt0WihXPVcVg)\n",
    "● **[1W字，六十张图让小白也能搞懂Word2vec ](https://mp.weixin.qq.com/s/PTRx8JhjCkNLfBh-G2IJDA)\n",
    "● [万物皆可Vector之语言模型：从N-Gram到NNLM、RNNLM](https://mp.weixin.qq.com/s/XiHzsjTK0TpQSjaOC6qFjw)\n",
    "\n",
    "● [万物皆可Vector之Word2vec：2个模型、2个优化及实战使用](https://mp.weixin.qq.com/s/hoQXBo2r4WGgxIGtODZkFw)\n",
    "\n",
    "● [【NLP修炼系列之词向量（二）】详解Word2Vec原理篇](https://mp.weixin.qq.com/s/krpBy8MeXX7twtCDEXsWhQ)\n",
    "\n",
    "\n",
    "## 代码学习\n",
    "● [word2vec(词嵌入)原理及代码实现](https://zhuanlan.zhihu.com/p/476920885)\n",
    "● [word2vec-include-datapreprocess](https://github.com/ttb1534/word2vec-include-datapreprocess)\n",
    "\n",
    "● [Word2vec](https://github.com/SeanLee97/nlp_learning/tree/master/word2vec)"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "mynlp_project",
   "language": "python",
   "display_name": "MyNLP_Project"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
