{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# FastText\n",
    "\n",
    "FastText是由Facebook的人工智能研究团队开发的一种用于文本分类和词向量表示的开源库，适用于大规模数据集和高性能需求的任务。FastText的相关论文由Piotr Bojanowski等人在2016年发表，FastText的作者认为之前的文本向量化方法没有考虑单词内部的子词之间的关系，所以性能受限。为了改善这个限制，FastText的核心原理包括两大部分：**词向量表示和文本分类**。\n",
    "\n",
    "## 词向量表示\n",
    "\n",
    "FastText 的一个关键创新在于利用了子词信息（Subword Information），即将每个单词表示为多个 n-gram 的组合。\n",
    "\n",
    "\n",
    "例如，单词 \"where\" 可以分解为\"wh\", \"whe\", \"her\", \"ere\", \"re\"等 n-gram。这种方法使得模型能够捕捉到词的子词信息，从而更好地处理未登录词和拼写错误。FastText 使用基于在上文中提到的Skip-Gram的方法来训练embedding。值得注意的是，FastText不仅使用单词本身，还使用单词的所有n-gram。每个单词被表示为它所有n-gram向量的组合。为了提高计算效率，FastText 使用分层Softmax代替传统的Softmax。分层Softmax将所有类别组织成一棵二叉树，每个叶节点对应一个类别。通过减少类别数量的对数级计算，分层Softmax极大地加速了训练和预测过程。\n",
    "\n",
    "## 文本分类原理\n",
    "\n",
    "在文本分类任务中，FastText通过将文本表示为一系列词向量的平均来生成文档向量。对于一个文本d中的每个词w，其向量表示为vw。文本d的表示为所有词向量的平均，即：\n",
    "$$\n",
    "v_d = \\frac{1}{|d|} \\sum_{w \\in d} v_w\n",
    "$$\n",
    "> 使用fastText进行文本分类的同时也会产生词的embedding，即embedding是fastText分类的产物\n",
    "\n",
    "FastText使用线性分类器来进行文本分类。文本向量vd通过一个全连接层，并使用Softmax函数来输出类别概率。分类模型的公式为\n",
    "$$\n",
    "y=Softmax(W \\cdot v_d + b)\n",
    "$$\n",
    "其中W是权重矩阵，b是偏置项\n",
    "\n",
    "### **fastText文本分类的核心思想是什么？**\n",
    "\n",
    "仔细观察模型的后半部分，即从隐含层输出到输出层输出，会发现它就是一个softmax线性多类别分类器，分类器的输入是一个用来表征当前文档的向量；模型的前半部分，即从输入层输入到隐含层输出部分，主要在做一件事情：生成用来表征文档的向量。那么它是如何做的呢？叠加构成这篇文档的所有词及n-gram的词向量，然后取平均。叠加词向量背后的思想就是传统的词袋法，即将文档看成一个由词构成的集合。\n",
    "于是fastText的核心思想就是：**将整篇文档的词及n-gram向量叠加平均得到文档向量，然后使用文档向量做softmax多分类。这中间涉及到两个技巧：字符级n-gram特征的引入以及分层Softmax分类。**\n",
    "\n",
    "### FastText与Word2Vec的区别？\n",
    "\n",
    "● **词汇表外（OOV）单词的处理**：\n",
    "Word2Vec： Word2Vec 在单词级别运行，为各个单词生成嵌入。它难以处理词汇表之外的单词，因为它无法代表训练期间未见过的单词。\\\n",
    "fastText：相比之下，fastText 通过将单词视为由字符 n-gram 组成来引入子词嵌入。这使得它能够通过将术语分解为子词单元并为这些单元生成嵌入来有效地处理词汇表外的单词，甚至对于没有见过的单词也是如此。此功能使 fastText 在处理罕见或形态复杂的表达式时更加稳健。\n",
    "\n",
    "● **单词的表示**\n",
    "Word2Vec： Word2Vec 仅基于单词生成单词嵌入，而不考虑内部结构或形态信息 \\\n",
    "fastText： fastText 捕获子词信息，使其能够根据组成字符 n 元语法来理解单词含义。这使得 fastText 能够通过考虑单词的形态构成来表示单词，从而提供更丰富的表示，特别是对于形态丰富的语言或具有专门术语的领域。\n",
    "\n",
    "● **训练效率**\n",
    "Word2Vec： Word2Vec 中的训练过程比旧方法相对更快，但由于其字级方法，可能比 fastText 慢。 \\\n",
    "fastText： fastText 以其卓越的速度和可扩展性而闻名，特别是在处理大型数据集时，因为它在子词级别上高效运行。\n",
    "\n",
    "● **使用场景**\n",
    "Word2Vec： Word2Vec 的单词级嵌入非常适合查找相似单词、理解单词之间的关系以及捕获语义相似性等任务 \\\n",
    "fastText： fastText 的子词嵌入使其更适合涉及词汇外单词、情感分析、语言识别以及需要更深入理解形态的任务的场景。\n",
    "## 优缺点\n",
    "优点：\n",
    "\n",
    "● 训练和预测速度也很快，适合实时应用。\n",
    "● 能够处理具有大量类别的任务，并保持高效性。\n",
    "● 支持多种语言的文本处理。\n",
    "\n",
    "缺点：\n",
    "● 使用静态词向量，无法捕捉上下文相关的词义变化。\n",
    "\n",
    "## 应用场景\n",
    "● 文本分类\n",
    "● 关键词提取\n",
    "● 拼写错误纠正"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "from gensim.models.fasttext import FastText\n",
    "import jieba\n",
    "import re\n",
    "import os\n",
    "import time"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "def read_stop(stop_words_path):\n",
    "    \"\"\"\n",
    "    读取停用词\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    # 读取停用词\n",
    "    stop_words = []\n",
    "    with open(stop_words_path, \"r\", encoding=\"utf-8\") as f_reader:\n",
    "        for line in f_reader:\n",
    "            line = line.replace(\"\\r\", \"\").replace(\"\\n\", \"\").strip()\n",
    "            stop_words.append(line)\n",
    "    # print(len(stop_words))\n",
    "    stop_words = set(stop_words)\n",
    "    # print(len(stop_words))\n",
    "    return stop_words"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "def data_process(stop_words, data_path, split_data_path):\n",
    "    \"\"\"\n",
    "    数据预处理\n",
    "    :return:\n",
    "    \"\"\"\n",
    "\n",
    "    # 文本预处理\n",
    "    sentecnces = []\n",
    "    rules = u\"[\\u4e00-\\u9fa5]+\"\n",
    "    pattern = re.compile(rules)\n",
    "    f_writer = open(split_data_path, \"w\", encoding=\"utf-8\")\n",
    "\n",
    "    with open(data_path, \"r\", encoding=\"utf-8\") as f_reader:\n",
    "        for line in f_reader:\n",
    "            line = line.replace(\"\\r\", \"\").replace(\"\\n\", \"\").strip()\n",
    "            if line == \"\" or line is None:\n",
    "                continue\n",
    "            line = \" \".join(jieba.cut(line))\n",
    "            seg_list = pattern.findall(line)\n",
    "            word_list = []\n",
    "            for word in seg_list:\n",
    "                if word not in stop_words:\n",
    "                    word_list.append(word)\n",
    "            if len(word_list) > 0:\n",
    "                sentecnces.append(word_list)\n",
    "                line = \" \".join(word_list)\n",
    "                f_writer.write(line + \"\\n\")\n",
    "                f_writer.flush()\n",
    "    f_writer.close()\n",
    "    return sentecnces"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10961\n"
     ]
    },
    {
     "data": {
      "text/plain": "[['书名', '天龙八部'],\n ['作者', '金庸'],\n ['本文', '早安', '电子书', '网友', '分享', '版权', '原作者'],\n ['用于', '商业行为', '后果自负'],\n ['早安', '电子书'],\n ['金庸', '作品集', '三联', '版', '序'],\n ['小学',\n  '时',\n  '爱读',\n  '课外书',\n  '低年级',\n  '时看',\n  '儿童',\n  '画报',\n  '小朋友',\n  '小学生',\n  '内容',\n  '小朋友',\n  '文库',\n  '似懂非懂',\n  '阅读',\n  '各种各样',\n  '章回小说',\n  '五六年',\n  '级',\n  '时',\n  '看新',\n  '文艺作品',\n  '喜爱',\n  '古典文学',\n  '作品',\n  '多于',\n  '近代',\n  '当代',\n  '新文学',\n  '个性',\n  '使然',\n  '朋友',\n  '喜欢',\n  '新文学',\n  '不爱',\n  '古典文学'],\n ['知识',\n  '当代',\n  '书报',\n  '中',\n  '寻求',\n  '小学',\n  '时代',\n  '得益',\n  '记忆',\n  '最深',\n  '爸爸',\n  '哥哥',\n  '购置',\n  '邹韬奋',\n  '所撰',\n  '萍踪',\n  '寄语',\n  '萍踪',\n  '忆语',\n  '世界各地',\n  '旅行',\n  '记',\n  '主编',\n  '生活',\n  '周报',\n  '新',\n  '旧',\n  '童年时代',\n  '深受',\n  '邹先生',\n  '生活',\n  '书店',\n  '之惠',\n  '生活',\n  '书店',\n  '三联书店',\n  '组成部分',\n  '十多年',\n  '前',\n  '香港三联书店',\n  '签',\n  '合同',\n  '中国',\n  '大陆',\n  '地区',\n  '出版',\n  '小说',\n  '因事',\n  '未果',\n  '重',\n  '行',\n  '筹划',\n  '三联书店',\n  '独家',\n  '出版',\n  '中国',\n  '大陆',\n  '地区',\n  '简体字',\n  '感到',\n  '欣慰',\n  '回忆',\n  '昔日',\n  '心中',\n  '充满',\n  '温馨',\n  '之意'],\n ['撰写',\n  '这套',\n  '总数',\n  '三十六',\n  '册',\n  '作品集',\n  '是从',\n  '一九五五年',\n  '七二年',\n  '约',\n  '十三',\n  '四年',\n  '包括',\n  '十二部',\n  '长篇小说',\n  '两篇',\n  '中篇小说',\n  '一篇',\n  '短篇小说',\n  '一篇',\n  '历史',\n  '人物',\n  '评传',\n  '若干篇',\n  '历史',\n  '考据',\n  '文字',\n  '出版',\n  '过程',\n  '奇怪',\n  '香港',\n  '台湾',\n  '海外',\n  '地区',\n  '中国',\n  '大陆',\n  '先出',\n  '各种各样',\n  '翻版',\n  '盗印',\n  '出版',\n  '校订',\n  '授权',\n  '正',\n  '版本',\n  '中国',\n  '大陆',\n  '三联',\n  '版',\n  '出版',\n  '天津',\n  '百花文艺出版社',\n  '一家',\n  '授权',\n  '出版',\n  '书剑',\n  '恩仇录',\n  '校印',\n  '依足',\n  '合同',\n  '支付',\n  '版税',\n  '依足',\n  '法例',\n  '缴付',\n  '所得税',\n  '余数',\n  '捐给',\n  '几家',\n  '文化',\n  '机构',\n  '支助',\n  '围棋',\n  '活动',\n  '这是',\n  '愉快',\n  '经验',\n  '未经',\n  '授权'],\n ['不付',\n  '版税',\n  '版本',\n  '粗制滥造',\n  '错讹',\n  '百出',\n  '借用',\n  '金庸',\n  '之名',\n  '撰写',\n  '出版',\n  '武侠小说',\n  '写',\n  '不敢掠美',\n  '充满',\n  '无聊',\n  '打斗',\n  '色情',\n  '描写',\n  '之作',\n  '令人',\n  '不快',\n  '出版社',\n  '翻印',\n  '香港',\n  '台湾',\n  '作家',\n  '作品',\n  '而用',\n  '笔名',\n  '出版发行',\n  '收到',\n  '无数',\n  '读者',\n  '来信',\n  '揭露',\n  '大表',\n  '愤慨',\n  '三联',\n  '版',\n  '发行',\n  '制止',\n  '种种',\n  '讲',\n  '道义',\n  '侠义',\n  '小说',\n  '主旨',\n  '讲',\n  '是非',\n  '讲',\n  '道义',\n  '太',\n  '过份']]"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir_current = os.getcwd()\n",
    "# 注意此处的路径，在 jupyter_notebook_config.py 中设置，否则会报错\n",
    "stop_words_file = os.path.join(dir_current, \"learning_word-vector/data/stop_words.txt\")\n",
    "data_file = os.path.join(dir_current, \"learning_word-vector/data/天龙八部.txt\")\n",
    "split_data_file = os.path.join(dir_current, \"learning_word-vector/data/天龙八部_split.txt\")\n",
    "\n",
    "stop_words = read_stop(stop_words_file)\n",
    "sentences = data_process(stop_words, data_file, split_data_file)\n",
    "print(len(sentences))\n",
    "sentences[:10]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始训练模型...\n",
      "训练模型结束，耗时： 24.37697434425354\n",
      "保存模型到： E:\\Pycharm_Data\\Pycharm_Project\\MyNLP_Project\\NLP-Learning-Workshop\\learning_word-vector/models/天龙八部_fasttext_Skip-gram.bin\n"
     ]
    }
   ],
   "source": [
    "# 模型训练\n",
    "start_time = time.time()\n",
    "print(\"开始训练模型...\")\n",
    "SG = 1 # 0表示CBOW，1表示Skip-gram\n",
    "\n",
    "model = FastText(sentences, vector_size=128, epochs=50, window=5, min_count=6,\n",
    "                          workers=10, sg=SG,\n",
    "                          negative=5, hs=0, seed=42,\n",
    "                          )\n",
    "\n",
    "print(\"训练模型结束，耗时：\", time.time() - start_time)\n",
    "# 保存模型\n",
    "if SG == 0:\n",
    "    model_name = \"天龙八部_fasttext_CBOW.bin\"\n",
    "else:\n",
    "    model_name = \"天龙八部_fasttext_Skip-gram.bin\"\n",
    "\n",
    "model_path = os.path.join(dir_current, f\"learning_word-vector/models/{model_name}\")\n",
    "print(\"保存模型到：\", model_path)\n",
    "\n",
    "model.save(model_path)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "那厮 0.5562141537666321\n",
      "徐长老 0.5247012972831726\n",
      "乔大爷 0.5245589017868042\n",
      "谭公 0.5162566304206848\n",
      "鲍千灵 0.49004656076431274\n",
      "鲍兄 0.48291927576065063\n",
      "玄苦大师 0.46966931223869324\n",
      "谭婆 0.4516589641571045\n",
      "赵钱孙 0.44818931818008423\n",
      "陈长老 0.44002845883369446\n",
      "乔峰与萧峰的相似度： 0.36966228\n",
      "['乔峰', '萧远山']与['慕容复', '慕容博']的相似度：\n",
      "\t 0.59094656\n"
     ]
    }
   ],
   "source": [
    "# 加载模型\n",
    "model2 = FastText.load(model_path)\n",
    "\n",
    "# 选出10个与乔峰最相近的10个词\n",
    "for e in model2.wv.most_similar(positive=[\"乔峰\"], topn=10):\n",
    "    print(e[0], e[1])\n",
    "\n",
    "# 计算两个词语的相似度\n",
    "sim_value = model2.wv.similarity('乔峰', '萧峰')\n",
    "print(\"乔峰与萧峰的相似度：\", sim_value)\n",
    "\n",
    "# 计算两个集合的相似度\n",
    "list1 = ['乔峰', '萧远山']\n",
    "list2 = ['慕容复', '慕容博']\n",
    "sim_value = model2.wv.n_similarity(list1, list2)\n",
    "print(\"['乔峰', '萧远山']与['慕容复', '慕容博']的相似度：\\n\\t\", sim_value)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 学习资源\n",
    "● [FastText](https://fasttext.cc/)\n",
    "\n",
    "● [FastText中文文档](https://fasttext.apachecn.org/#/)\n",
    "\n",
    "● [fastText原理及实践](https://mp.weixin.qq.com/s/cnu2xDFgsXIT0ObaXfZYHw)\n",
    "\n",
    "● [Welcome fastText to the Hugging Face Hub](https://huggingface.co/blog/fasttext)\n",
    "\n",
    "## 代码学习\n",
    "\n",
    "● [FastText](https://github.com/facebookresearch/fastText)\n",
    "● [Word representations](https://fasttext.cc/docs/en/unsupervised-tutorial.html)\n",
    "\n",
    "● [Word Embedding--Wiki(Word2vec、fastText)](https://github.com/lintseju/word_embedding)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
