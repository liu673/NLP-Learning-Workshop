{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# ELMo\n",
    "\n",
    "ELMo（Embeddings from Language Models）是一种由Allen Institute for AI的研究者Matthew E. Peters等在2018年提出的embedding方法，通过上下文中的词语来捕捉语义。ELMo的作者认为之前的文本向量化方式没有考虑一个词在不同上下文中意义的变化，所以与Word2Vec和GloVe不同，ELMo能够捕捉到词在不同上下文中的不同意义，从而生成更为丰富和精确的embedding。\n",
    "\n",
    "ELMo使用双向LSTM（Long Short-Term Memory）网络作为基础架构。该网络包括一个向前的LSTM和一个向后的LSTM，它们分别从左到右和从右到左遍历文本序列，学习每个词的前向和后向上下文。这个双向LSTM被训练为一个语言模型，目标是预测句子中的下一个词（对于前向LSTM）和前一个词（对于后向LSTM）。对于给定的文本序列，ELMo会为每个词提取多层的表示。对于每个词，ELMo提供三层的embedding输出：一层来自embedding层（类似于传统的embedding），另外两层来自双向LSTM的各自输出。最后，ELMo使用这些层的加权和作为结果，公式如下：\n",
    "$$\n",
    "{ELMO}_t = \\gamma \\sum_{j=0}^L s_j h_{t}^{(j)}\n",
    "$$\n",
    "其中 $\\gamma$ 是一个可学习的标量；$s_j$是层$j$的可学习权重；$h_t^{(j)}$是词 $t$ 在层 $j$ 的表示。为了使这些输出被应用在不同任务中，ELMo提出了一种动态加权的方法，可以为不同层的输出分配不同的权重。这样，模型不仅使用了所有层的信息，还根据任务动态调整了每层信息的重要性。\n",
    "\n",
    "\n",
    "## 优缺点\n",
    "优点：\n",
    "\n",
    "● 基于整个句子的上下文信息生成embedding，能够更好地处理多义词和上下文依赖性强的词。\n",
    "● 能够捕捉复杂的句子结构和长距离依赖关系。\n",
    "● 预训练的ELMo模型可以很容易地迁移到各种下游自然语言处理任务中。\n",
    "\n",
    "\n",
    "缺点：\n",
    "\n",
    "● 模型较大，训练和推理过程需要大量的计算资源。\n",
    "● 由于模型复杂，推理速度较慢。\n",
    "\n",
    "\n",
    "\n",
    "## 应用场景\n",
    "\n",
    "● 情感分析\n",
    "● 问答系统\n",
    "● 文本分类\n",
    "● 机器翻译\n"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
